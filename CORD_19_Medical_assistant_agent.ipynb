{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Loiruck/CORD-19-LLM-Agent/blob/main/CORD_19_Medical_assistant_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WzDzKYt7tkK"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzhtVsoo-2cL"
      },
      "source": [
        "# LLM Agent for the CORD-19 Dataset.\n",
        "\n",
        "Navigating the science behind COVID-19 and its connection to smoking or high blood pressure can be a lot. That's why this AI agent was built! Think of it as a knowledgeable guide that's been trained on the CORD-19 research dataset. It's here to help answer your questions by pulling out relevant details directly from the scientific papers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "340ftwAh-5aS"
      },
      "source": [
        "# SECTION A: SETUP AND DEPENDENCIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEpafAld_E9C"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index llama-index-embeddings-huggingface llama-index-llms-huggingface bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "061P23RY_Flq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core import Document\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core import load_index_from_storage\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "# Add these to your import block\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core.memory import ChatMemoryBuffer\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x4B0pz3_4hZ"
      },
      "source": [
        "# SECTION B: DATA ACQUISITION AND INITIAL LOADING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdEcJNeT_Qi6",
        "outputId": "91046549-7213-462a-bdb1-505ca9396c8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/dataset-metadata-for-cord19\n"
          ]
        }
      ],
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(handle=\"googleai/dataset-metadata-for-cord19\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2ktoElTSM81",
        "outputId": "744fedac-51eb-4077-ef5a-1cdf213173b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CORD19 datasets - Sheet 1.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "os.listdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d1BAkUyySQvA",
        "outputId": "6423599b-1ab1-4e19-cffe-8635bd8cb595"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/kaggle/input/dataset-metadata-for-cord19/CORD19 datasets - Sheet 1.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "filename_with_path = path + \"/\" + os.listdir(path)[0]\n",
        "filename_with_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joSSL56LSVDP"
      },
      "outputs": [],
      "source": [
        "df_meta_cord19 = pd.read_csv(filename_with_path)\n",
        "df_meta_cord19.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekQ2EHHGSW3-"
      },
      "outputs": [],
      "source": [
        "df_meta_cord19.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGLCEbQVANvN"
      },
      "source": [
        "# SECTION C: DATA CLEANING AND PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu-IXCiYSdqG",
        "outputId": "5f5b372f-8e97-4dbb-9274-12775969d10c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning and preparing 'description' column...\n",
            "Working with 14126 documents after initial cleaning.\n"
          ]
        }
      ],
      "source": [
        "# ===>>> ADD/REPLACE WITH THIS CODE (using 'description') <<<===\n",
        "print(\"Cleaning and preparing 'description' column...\")\n",
        "# Use 'description' as identified from your .info() output\n",
        "df_meta_cord19_cleaned = df_meta_cord19[df_meta_cord19['description'].notnull()].copy()\n",
        "df_meta_cord19_cleaned['description'] = df_meta_cord19_cleaned['description'].astype(str)\n",
        "# Optionally, prepare 'paper_title' if you plan to use it in metadata\n",
        "if 'paper_title' in df_meta_cord19_cleaned.columns:\n",
        "    df_meta_cord19_cleaned['paper_title'] = df_meta_cord19_cleaned['paper_title'].astype(str)\n",
        "print(f\"Working with {len(df_meta_cord19_cleaned)} documents after initial cleaning.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3Cq5_vXAXOi"
      },
      "source": [
        "# SECTION D: CONTENT-BASED DOCUMENT FILTERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6P2aY0oVk5x",
        "outputId": "a425253b-a097-479e-d784-5e619c9aae13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering documents for relevance to smoking or high blood pressure...\n"
          ]
        }
      ],
      "source": [
        "# ===>>> ADD/REPLACE WITH THIS CODE (using 'description') <<<===\n",
        "print(\"Filtering documents for relevance to smoking or high blood pressure...\")\n",
        "keywords_smoking = ['smoking', 'cigarette', 'nicotine', 'tobacco', 'vaping']\n",
        "keywords_hbp = ['high blood pressure', 'hypertension', 'bp']\n",
        "\n",
        "# Ensure you are searching in the 'description' column of df_meta_cord19_cleaned\n",
        "contains_smoking = df_meta_cord19_cleaned['description'].str.contains('|'.join(keywords_smoking), case=False, na=False)\n",
        "contains_hbp = df_meta_cord19_cleaned['description'].str.contains('|'.join(keywords_hbp), case=False, na=False)\n",
        "\n",
        "df_relevant_docs = df_meta_cord19_cleaned[contains_smoking | contains_hbp].copy()\n",
        "print(f\"Found {len(df_relevant_docs)} documents relevant to smoking or high blood pressure.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR2SpQikAjDR"
      },
      "source": [
        "# SECTION E: EMBEDDING MODEL INITIALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UE4NVzycHpJs"
      },
      "outputs": [],
      "source": [
        "# Add this before creating the LLM or the index\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# Initialize the embedding model (you can choose other models from Hugging Face)\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "Settings.embed_model = embed_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0zwH0BM_V0K"
      },
      "source": [
        "## SECTION F: VECTOR DATABASE USING THE LLAMAINDEX FUNCTION LIBRARY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSR6xviX_zQC"
      },
      "outputs": [],
      "source": [
        "# \"\"\"## Create a vector database using the LlamaIndex function library.\"\"\" # This is a markdown heading\n",
        "\n",
        "# --- REPLACE THE DOCUMENT CREATION LOGIC BELOW ---\n",
        "# The old logic uses 'df_meta_cord19_filtered' which is not defined after the cleanup,\n",
        "# and it also directly uses 'description' without referencing the topic-filtered DataFrame.\n",
        "\n",
        "# Convert pandas DataFrame to LlamaIndex Documents\n",
        "# We'll use the 'description' column for the main content.\n",
        "# You can also add other relevant fields to metadata if needed.\n",
        "\n",
        "# ===>>> REPLACE THE FOLLOWING LOOP:\n",
        "# documents = []\n",
        "# for index, row in tqdm(df_meta_cord19_filtered.iterrows(), total=df_meta_cord19_filtered.shape[0], desc=\"Creating Documents\"):\n",
        "#     # Ensure the text content is a string\n",
        "#     text_content = str(row['description']) if pd.notnull(row['description']) else \"\"\n",
        "#     documents.append(Document(text=text_content))\n",
        "# print(f\"Created {len(documents)} documents.\")\n",
        "# ===>>> WITH THIS CORRECTED LOOP (using df_relevant_docs and 'description'):\n",
        "documents = []\n",
        "if not df_relevant_docs.empty:\n",
        "    print(f\"Creating LlamaIndex Documents from {len(df_relevant_docs)} relevant rows...\") # Adapted from previous guidance\n",
        "    for index, row in tqdm(df_relevant_docs.iterrows(), total=df_relevant_docs.shape[0], desc=\"Creating Documents\"): # Iterate over df_relevant_docs\n",
        "        text_content = str(row['description']) # Get text from 'description' column\n",
        "        metadata = {}\n",
        "        if 'paper_title' in row and pd.notnull(row['paper_title']): # Use 'paper_title' from df_relevant_docs\n",
        "            metadata[\"title\"] = str(row['paper_title'])\n",
        "        documents.append(Document(text=text_content, metadata=metadata))\n",
        "else:\n",
        "    print(\"No relevant documents found after filtering. The index will be empty.\")\n",
        "\n",
        "print(f\"Created {len(documents)} LlamaIndex Documents.\")\n",
        "# --- END OF REPLACEMENT ---\n",
        "\n",
        "# Create the VectorStoreIndex - This part is fine if 'documents' is correctly populated\n",
        "print(\"Creating the VectorStoreIndex.This might take a while...\")\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    show_progress=True\n",
        ")\n",
        "print(\"VectorStoreIndex created successfully.\")\n",
        "\n",
        "# ... (Optional persist/load index code is fine) ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UfEF0iC_zm9"
      },
      "source": [
        "## SECTION G: LANGUAGE MODEL (LLM) INITIALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMh5Jjk4_XxD"
      },
      "outputs": [],
      "source": [
        "# Létrehozzuk a nyelvi modellt (LLM), amit az ágens fog használni.\n",
        "llm = HuggingFaceLLM(\n",
        "    model_name=\"colesmcintosh/Llama-3.2-1B-Instruct-Mango\",       # Nyelvi modell beállítása\n",
        "    tokenizer_name=\"colesmcintosh/Llama-3.2-1B-Instruct-Mango\",   # Nyelvi modell tokenizátorának beállítása\n",
        "    context_window=2048,                                          # Maximum token limit\n",
        "    max_new_tokens=256,                                           # Válasz maximális hossza\n",
        "    device_map=\"cuda:0\",                                          # GPU használata,\n",
        "    generate_kwargs={\"temperature\": 0.95, \"do_sample\": True},     # Ezek a paraméterek befolyásolják a modell válaszainak véletlenszerűségét és kreativitását.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv6qvCva_5Ed"
      },
      "outputs": [],
      "source": [
        "Settings.llm = llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nulfpfkBUSl"
      },
      "source": [
        "# SECTION H: CHAT ENGINE CONFIGURATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omxOcLxp_9ob"
      },
      "outputs": [],
      "source": [
        "# Létrehozzuk a chat motort, ami az ágens párbeszédéért felelős.\n",
        "chat_engine = index.as_chat_engine(\n",
        "    # Ez a paraméter beállítja, hogy a chat motor a korábban létrehozott vektoradatbázist használja a válaszokhoz.\n",
        "    chat_mode=\"context\",\n",
        "    # Ez a paraméter beállítja a chat motor memóriáját. A ChatMemoryBuffer emlékszik a korábbi beszélgetésekre.\n",
        "    memory=ChatMemoryBuffer.from_defaults(token_limit=32000),\n",
        "    # Ez a paraméter beállítja a rendszerüzenetet, ami az ágens viselkedését befolyásolja. Ebben az esetben az ágens egy orvosi chatbot, amely a MedQuad adathalmaz alapján válaszol.\n",
        "    system_prompt=(\n",
        "    \"You are a specialized medical information assistant. \"\n",
        "    \"Your knowledge is strictly limited to information about the relationship between COVID-19 and smoking,and COVID-19 and high blood pressure, based on the provided CORD-19 research documents. \"\n",
        "\n",
        ")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5Fx8lLhBfot"
      },
      "source": [
        "# SECTION I: WEB APPLICATION (FLASK SETUP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwKqs7pCE2-L"
      },
      "outputs": [],
      "source": [
        "# Ensure this cell is run\n",
        "!pip install Flask flask-cors # Run this first if you haven't in this session or after a restart\n",
        "!pip install flask-ngrok\n",
        "\n",
        "# ... (your existing imports)\n",
        "from flask import Flask\n",
        "from flask_ngrok import run_with_ngrok # Add this import\n",
        "from llama_index.core import Settings\n",
        "# ... (all other llama_index, HuggingFace, etc. imports) ...\n",
        "\n",
        "from flask import Flask, render_template_string, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import threading # <<< --- ADD THIS LINE HERE ---\n",
        "try:\n",
        "    from google.colab.output import eval_js # For getting Colab URL\n",
        "except ImportError:\n",
        "    eval_js = None # Define it as None if not in Colab\n",
        "\n",
        "print(\"All required modules have been imported/defined.\")\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app) # Add this line before your routes\n",
        "CORS(app)\n",
        "\n",
        "# ... (your @app.route definitions and FRONTEND_HTML) ...\n",
        "# --- REPLACE YOUR OLD if __name__ == '__main__': BLOCK WITH THIS ---\n",
        "if __name__ == '__main__':\n",
        "    print(\"Preparing to start Flask app...\")\n",
        "\n",
        "    # Run Flask app in a separate thread\n",
        "    # This allows the Colab URL to be printed after the server starts listening\n",
        "    flask_thread = threading.Thread(target=app.run, kwargs={'host':'0.0.0.0','port':5000, 'debug':False, 'use_reloader':False})\n",
        "    flask_thread.start()\n",
        "    print(\"Flask app thread started. It might take a few seconds for the server to be ready.\")\n",
        "\n",
        "    # Get and print the Colab proxy URL (if in Colab)\n",
        "    if eval_js: # Checks if eval_js was imported (i.e., if running in Colab)\n",
        "        print(\"Attempting to get Colab proxy URL...\")\n",
        "        # Add a small delay to give the server time to start before trying to get the proxy URL\n",
        "        import time\n",
        "        time.sleep(5) # Wait 5 seconds\n",
        "        try:\n",
        "            colab_url = eval_js('google.colab.kernel.proxyPort(5000)')\n",
        "            print(f\"Your Colab app should be accessible at: {colab_url}\")\n",
        "            print(\"If the link doesn't work immediately, please wait a few more seconds for the server to fully initialize.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not get Colab URL via eval_js: {e}\")\n",
        "            print(\"You might need to manually check Colab's output for a forwarded port if the Flask server messages appear.\")\n",
        "    else:\n",
        "        print(\"Not running in Colab or google.colab.output not available.\")\n",
        "        print(f\"If running locally, try accessing the app via http://127.0.0.1:5000 or http://0.0.0.0:5000\")\n",
        "\n",
        "    print(\"Main script setup complete. Flask server is running in a background thread.\")\n",
        "    print(\"You can now try accessing the URL provided above.\")\n",
        "# --- END OF REPLACEMENT ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkLxOZEdTF_X"
      },
      "source": [
        "# SECTION J: WEB APPLICATION, FRONTEND HTML AND JAVASCRIPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXGJLWofK-61"
      },
      "outputs": [],
      "source": [
        "# --- START OF FLASK WEB FRONTEND ADDITION ---\n",
        "app = Flask(__name__)\n",
        "CORS(app) # Enable Cross-Origin Resource Sharing\n",
        "\n",
        "FRONTEND_HTML = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Medical Chatbot</title>\n",
        "    <style>\n",
        "        body { font-family: Arial, sans-serif; margin: 0; padding: 0; background-color: #f0f2f5; display: flex; flex-direction: column; align-items: center; min-height: 100vh; }\n",
        "        header { background-color: #007bff; color: white; padding: 15px 0; text-align: center; width: 100%; box-shadow: 0 2px 5px rgba(0,0,0,0.1); position: fixed; top: 0; left: 0; z-index: 1000;}\n",
        "        header h1 { margin: 0; font-size: 1.6em; }\n",
        "        #chat-container { width: 95%; max-width: 800px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); margin-top: 100px; /* Adjusted for fixed header */ margin-bottom: 20px; display: flex; flex-direction: column; flex-grow: 1; overflow: hidden; height: calc(100vh - 140px); /* Adjusted for fixed header and input */ }\n",
        "        #chatbox { flex-grow: 1; padding: 20px; overflow-y: auto; border-bottom: 1px solid #e0e0e0; }\n",
        "        .message { margin-bottom: 12px; padding: 10px 15px; border-radius: 18px; line-height: 1.5; max-width: 80%; word-wrap: break-word; }\n",
        "        .user-message { background-color: #007bff; color: white; margin-left: auto; border-bottom-right-radius: 5px; align-self: flex-end; }\n",
        "        .agent-message { background-color: #e9ecef; color: #212529; margin-right: auto; border-bottom-left-radius: 5px; align-self: flex-start; }\n",
        "        .system-message { background-color: #fff3cd; color: #856404; text-align: center; font-style: italic; padding: 8px; border-radius: 5px; }\n",
        "        #input-area { display: flex; padding: 15px; border-top: 1px solid #e0e0e0; background-color: #f8f9fa;}\n",
        "        #queryInput { flex-grow: 1; padding: 10px 15px; border: 1px solid #ced4da; border-radius: 20px; margin-right: 10px; font-size: 0.95em; }\n",
        "        #queryInput:focus { border-color: #80bdff; box-shadow: 0 0 0 0.2rem rgba(0,123,255,.25); outline: none; }\n",
        "        .button { padding: 10px 18px; color: white; border: none; border-radius: 20px; cursor: pointer; font-size: 0.95em; transition: background-color 0.2s ease-in-out; }\n",
        "        #sendButton { background-color: #007bff; }\n",
        "        #sendButton:hover { background-color: #0056b3; }\n",
        "        #resetButton { background-color: #6c757d; margin-left: 8px; }\n",
        "        #resetButton:hover { background-color: #545b62; }\n",
        "        .thinking { font-style: italic; color: #6c757d; padding: 8px 0; text-align: left; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <header><h1>Medical Chatbot (CORD-19)</h1></header>\n",
        "    <div id=\"chat-container\">\n",
        "        <div id=\"chatbox\">\n",
        "            <div class=\"message agent-message\">Hello! Ask me about COVID-19 in relation to smoking or high blood pressure.</div>\n",
        "        </div>\n",
        "        <div id=\"input-area\">\n",
        "            <input type=\"text\" id=\"queryInput\" placeholder=\"Type your question...\" onkeypress=\"handleKeyPress(event)\">\n",
        "            <button id=\"sendButton\" class=\"button\" onclick=\"askQuestion()\">Send</button>\n",
        "            <button id=\"resetButton\" class=\"button\" onclick=\"resetChat()\">Reset</button>\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "        const chatbox = document.getElementById('chatbox');\n",
        "        const queryInput = document.getElementById('queryInput');\n",
        "        const sendButton = document.getElementById('sendButton');\n",
        "        let thinkingDiv = null; // To hold the \"thinking...\" message\n",
        "\n",
        "        function addMessage(text, type) {\n",
        "            const messageDiv = document.createElement('div');\n",
        "            messageDiv.classList.add('message', type + '-message');\n",
        "            messageDiv.textContent = text; // Using textContent for security\n",
        "            chatbox.appendChild(messageDiv);\n",
        "            chatbox.scrollTop = chatbox.scrollHeight; // Auto-scroll\n",
        "        }\n",
        "\n",
        "        function showThinking() {\n",
        "            if (thinkingDiv) return; // Already showing\n",
        "            thinkingDiv = document.createElement('div');\n",
        "            thinkingDiv.classList.add('thinking', 'agent-message'); // Style like an agent message\n",
        "            thinkingDiv.textContent = 'Agent is thinking...';\n",
        "            chatbox.appendChild(thinkingDiv);\n",
        "            chatbox.scrollTop = chatbox.scrollHeight;\n",
        "        }\n",
        "\n",
        "        function hideThinking() {\n",
        "            if (thinkingDiv) {\n",
        "                chatbox.removeChild(thinkingDiv);\n",
        "                thinkingDiv = null;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        async function askQuestion() {\n",
        "            const query = queryInput.value.trim();\n",
        "            if (!query) return;\n",
        "\n",
        "            addMessage(query, 'user');\n",
        "            queryInput.value = '';\n",
        "            queryInput.disabled = true;\n",
        "            sendButton.disabled = true;\n",
        "            showThinking();\n",
        "\n",
        "            try {\n",
        "                const response = await fetch('/chat', {\n",
        "                    method: 'POST',\n",
        "                    headers: { 'Content-Type': 'application/json' },\n",
        "                    body: JSON.stringify({ query: query })\n",
        "                });\n",
        "\n",
        "                hideThinking();\n",
        "\n",
        "                if (!response.ok) {\n",
        "                    const errorData = await response.json().catch(() => ({ error: \"Failed to parse error response.\" }));\n",
        "                    addMessage('Error: ' + (errorData.error || 'Failed to get response from server.'), 'agent');\n",
        "                    return;\n",
        "                }\n",
        "                const data = await response.json();\n",
        "                addMessage(data.response, 'agent');\n",
        "\n",
        "            } catch (error) {\n",
        "                hideThinking();\n",
        "                console.error('Error during fetch:', error);\n",
        "                addMessage('Error: Could not connect to the agent. Please check the server.', 'agent');\n",
        "            } finally {\n",
        "                 queryInput.disabled = false;\n",
        "                 sendButton.disabled = false;\n",
        "                 queryInput.focus();\n",
        "            }\n",
        "        }\n",
        "\n",
        "        async function resetChat() {\n",
        "            addMessage('Resetting chat history...', 'system');\n",
        "            queryInput.disabled = true;\n",
        "            sendButton.disabled = true;\n",
        "\n",
        "            try {\n",
        "                const response = await fetch('/reset', {\n",
        "                    method: 'POST',\n",
        "                    headers: { 'Content-Type': 'application/json' },\n",
        "                });\n",
        "                if (!response.ok) {\n",
        "                    const errorData = await response.json().catch(() => ({ error: \"Failed to parse error response.\" }));\n",
        "                    addMessage('Error: ' + (errorData.error || 'Failed to reset.'), 'agent');\n",
        "                    return;\n",
        "                }\n",
        "                const data = await response.json();\n",
        "                addMessage(data.response, 'agent');\n",
        "                // Optionally, you could clear more messages from the frontend if desired:\n",
        "                // chatbox.innerHTML = \"\"; // Clears all messages\n",
        "                // addMessage(data.response, 'agent'); // Then add the confirmation\n",
        "            } catch (error) {\n",
        "                 console.error('Error during reset:', error);\n",
        "                addMessage('Error: Could not connect to reset chat.', 'agent');\n",
        "            } finally {\n",
        "                 queryInput.disabled = false;\n",
        "                 sendButton.disabled = false;\n",
        "                 queryInput.focus();\n",
        "            }\n",
        "        }\n",
        "\n",
        "        function handleKeyPress(event) {\n",
        "            if (event.key === 'Enter') {\n",
        "                askQuestion();\n",
        "            }\n",
        "        }\n",
        "        queryInput.focus(); // Focus on input on load\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template_string(FRONTEND_HTML)\n",
        "\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat_handler_route():\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        query = data.get('query')\n",
        "\n",
        "        if not query:\n",
        "            return jsonify({\"error\": \"No query provided\"}), 400\n",
        "\n",
        "        # Use the globally defined chat_engine\n",
        "        response_stream = chat_engine.stream_chat(query)\n",
        "        full_response = \"\".join(token for token in response_stream.response_gen)\n",
        "\n",
        "        return jsonify({\"response\": full_response})\n",
        "    except Exception as e:\n",
        "        print(f\"Error in /chat endpoint: {str(e)}\") # Log error to server console\n",
        "        return jsonify({\"error\": \"An internal server error occurred.\"}), 500\n",
        "\n",
        "@app.route('/reset', methods=['POST'])\n",
        "def reset_handler_route():\n",
        "    try:\n",
        "        chat_engine.reset() # Reset the chat engine's memory\n",
        "        return jsonify({\"response\": \"Chat history has been successfully reset.\"})\n",
        "    except Exception as e:\n",
        "        print(f\"Error in /reset endpoint: {str(e)}\") # Log error to server console\n",
        "        return jsonify({\"error\": \"An internal server error occurred during reset.\"}), 500\n",
        "\n",
        "# --- END OF FLASK WEB FRONTEND ADDITION ---\n",
        "\n",
        "# Remove or comment out the old command-line loop:\n",
        "# while True:\n",
        "#   query = input(\"> \")\n",
        "#   if query.lower() == \"quit\":\n",
        "#       break\n",
        "#   print(\"Agent: \", end=\"\", flush=True)\n",
        "#   response = chat_engine.stream_chat(query)\n",
        "#   for token in response.response_gen:\n",
        "#       print(token, end=\"\", flush=True)\n",
        "#   print()\n",
        "# chat_engine.reset() # This line (if it was at the end of the script) is no longer needed here.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # If running in Google Colab, you can use this to get a public URL:\n",
        "    # from google.colab.output import eval_js\n",
        "    # print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))\n",
        "\n",
        "    # For local development or other environments:\n",
        "    print(\"Starting Flask app on http://127.0.0.1:5000/\")\n",
        "    app.run(host='0.0.0.0', port=5000, debug=False) # debug=True can be useful for development, but use False for stability\n",
        "                                                 # host='0.0.0.0' makes it accessible on your network"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}